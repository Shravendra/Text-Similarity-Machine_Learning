{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd1e49f",
   "metadata": {},
   "source": [
    "# Importing some Important library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc32f422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import collections\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer  # For Lemmetization of words\n",
    "from nltk.corpus import stopwords  # Load list of stopwords\n",
    "from nltk import word_tokenize # Convert paragraph in tokens\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from gensim.models import word2vec # For represent words in vectors\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fda57b6",
   "metadata": {},
   "source": [
    "# Reading Provided Data-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81b3ce21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of text_data :  (3000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>broadband challenges tv viewing the number of ...</td>\n",
       "      <td>gardener wins double in glasgow britain s jaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rap boss arrested over drug find rap mogul mar...</td>\n",
       "      <td>amnesty chief laments war failure the lack of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>player burn-out worries robinson england coach...</td>\n",
       "      <td>hanks greeted at wintry premiere hollywood sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>hearts of oak 3-2 cotonsport hearts of oak set...</td>\n",
       "      <td>redford s vision of sundance despite sporting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sir paul rocks super bowl crowds sir paul mcca...</td>\n",
       "      <td>mauresmo opens with victory in la amelie maure...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID                                              text1  \\\n",
       "0          0  broadband challenges tv viewing the number of ...   \n",
       "1          1  rap boss arrested over drug find rap mogul mar...   \n",
       "2          2  player burn-out worries robinson england coach...   \n",
       "3          3  hearts of oak 3-2 cotonsport hearts of oak set...   \n",
       "4          4  sir paul rocks super bowl crowds sir paul mcca...   \n",
       "\n",
       "                                               text2  \n",
       "0  gardener wins double in glasgow britain s jaso...  \n",
       "1  amnesty chief laments war failure the lack of ...  \n",
       "2  hanks greeted at wintry premiere hollywood sta...  \n",
       "3  redford s vision of sundance despite sporting ...  \n",
       "4  mauresmo opens with victory in la amelie maure...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read given data-set using pandas\n",
    "text_data = pd.read_csv(\"Precily_Text_Similarity.csv\")\n",
    "print(\"Shape of text_data : \", text_data.shape)\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1269bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking whether is it having NAN values or not.\n",
    "text_data.isna().values.any() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b30760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text1    0\n",
       "text2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.isnull().sum() # Check if text data have any null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5eba64",
   "metadata": {},
   "source": [
    "# Preprocessing of text1 & text2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12e4e504",
   "metadata": {},
   "source": [
    "✔ Convert phrases like won't to will not using function decontracted() below\n",
    "✔ Remove Stopwords.\n",
    "✔ Remove any special symbols and lower case all words\n",
    "✔ lemmatizing words using WordNetLemmatizer define in function word_tokenizer below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc057643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf04cd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3000/3000 [07:08<00:00,  7.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Combining all the above stundents \n",
    "\n",
    "preprocessed_text1 = []\n",
    "\n",
    "# tqdm is for printing the status bar\n",
    "\n",
    "for sentance in tqdm(text_data['text1'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords.words('english'))\n",
    "    preprocessed_text1.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b98dc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>broadband challenges tv viewing number europea...</td>\n",
       "      <td>gardener wins double in glasgow britain s jaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rap boss arrested drug find rap mogul marion s...</td>\n",
       "      <td>amnesty chief laments war failure the lack of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player burn worries robinson england coach and...</td>\n",
       "      <td>hanks greeted at wintry premiere hollywood sta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text1  \\\n",
       "0  broadband challenges tv viewing number europea...   \n",
       "1  rap boss arrested drug find rap mogul marion s...   \n",
       "2  player burn worries robinson england coach and...   \n",
       "\n",
       "                                               text2  \n",
       "0  gardener wins double in glasgow britain s jaso...  \n",
       "1  amnesty chief laments war failure the lack of ...  \n",
       "2  hanks greeted at wintry premiere hollywood sta...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging preprocessed_text1 in text_data\n",
    "text_data['text1'] = preprocessed_text1\n",
    "text_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42f09324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3000/3000 [07:20<00:00,  6.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Combining all the above stundents \n",
    "from tqdm import tqdm\n",
    "preprocessed_text2 = []\n",
    "\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(text_data['text2'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "   \n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords.words('english'))\n",
    "    preprocessed_text2.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1332aa0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>broadband challenges tv viewing number europea...</td>\n",
       "      <td>gardener wins double glasgow britain jason gar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rap boss arrested drug find rap mogul marion s...</td>\n",
       "      <td>amnesty chief laments war failure lack public ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player burn worries robinson england coach and...</td>\n",
       "      <td>hanks greeted wintry premiere hollywood star t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text1  \\\n",
       "0  broadband challenges tv viewing number europea...   \n",
       "1  rap boss arrested drug find rap mogul marion s...   \n",
       "2  player burn worries robinson england coach and...   \n",
       "\n",
       "                                               text2  \n",
       "0  gardener wins double glasgow britain jason gar...  \n",
       "1  amnesty chief laments war failure lack public ...  \n",
       "2  hanks greeted wintry premiere hollywood star t...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging preprocessed_text2 in text_data\n",
    "\n",
    "text_data['text2'] = preprocessed_text2\n",
    "\n",
    "text_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a690190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(text):\n",
    "            #tokenizes and stems the text\n",
    "            tokens = word_tokenize(text)\n",
    "            lemmatizer = WordNetLemmatizer() \n",
    "            tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "            return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa84b3f",
   "metadata": {},
   "source": [
    "# Proposed Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b8c64",
   "metadata": {},
   "source": [
    "# Word embeddings :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb9414",
   "metadata": {},
   "source": [
    "Word embeddings are low dimensional vectors obtained by training a neural network on a large corpus to predict a word given a context (Continuous Bag Of Words model) or to predict the context given a word (skip gram model). The context is a window of surrounding words. Pre-trained word embeddings are also available in the word2vec code.google page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ef075",
   "metadata": {},
   "source": [
    "In this i am using Google news pre trained vectors and compare similarity between text1 & text2 using n_similarity method in gensim library which is nothing compares cosine similarity between two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f0afbc",
   "metadata": {},
   "source": [
    "Considering it as a unsupervised problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d2146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pre_trained Google News Vectors after download file\n",
    "wordmodelfile=\"GoogleNews-vectors-negative300.bin\"\n",
    "wordmodel= gensim.models.KeyedVectors.load_word2vec_format(wordmodelfile, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b066863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code check if word in text1 & text2 present in our google news vectors vocabalry.\n",
    "# if not it removes that word and if present it compares similarity score between text1 and text2 words\n",
    "\n",
    "similarity = [] # List for store similarity score\n",
    "\n",
    "for ind in text_data.index:\n",
    "    \n",
    "        s1 = text_data['text1'][ind]\n",
    "        s2 = text_data['text2'][ind]\n",
    "        \n",
    "        if s1==s2:\n",
    "                 similarity.append(0.0) # 0 means highly similar\n",
    "                \n",
    "        else:   \n",
    "\n",
    "            s1words = word_tokenizer(s1)\n",
    "            s2words = word_tokenizer(s2)        \n",
    "            \n",
    "            vocab = wordmodel.key_to_index #the vocabulary considered in the word embeddings\n",
    "            \n",
    "            if len(s1words and s2words)==0:\n",
    "                    similarity.append(1.0)\n",
    "\n",
    "            else:\n",
    "                \n",
    "                for word in s1words.copy(): #remove sentence words not found in the vocab\n",
    "                    if (word not in vocab):\n",
    "                        \n",
    "                            s1words.remove(word)\n",
    "                                           \n",
    "                for word in s2words.copy(): #idem\n",
    "\n",
    "                    if (word not in vocab):\n",
    "                           \n",
    "                            s2words.remove(word)\n",
    "                                                       \n",
    "                similarity.append((1-wordmodel.n_similarity(s1words, s2words))) # as it is given 1 means highly dissimilar & 0 means highly similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923f5b2",
   "metadata": {},
   "source": [
    "# Final Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e5169",
   "metadata": {},
   "source": [
    "Make Final DataFrame and save a CSV file of similarity scores with Unique_ID. (Columns : Unique_ID, Similarity_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5046d81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.314564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.366671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.282510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID  Similarity_score\n",
       "0          0          0.314564\n",
       "1          1          0.366671\n",
       "2          2          0.282510"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Unique_ID and similarity\n",
    "final_score = pd.DataFrame({'Unique_ID':text_data.Unique_ID,'Similarity_score':similarity})\n",
    "final_score.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eba0ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE DF as CSV file \n",
    "final_score.to_csv('final_score.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd8b5bf",
   "metadata": {},
   "source": [
    "# THANKS !!!   END OF ASSIGNMENT "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
